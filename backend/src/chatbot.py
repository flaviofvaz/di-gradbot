from .ingestion.embeddings import BaseEmbedder
from .ingestion.vector_db import BaseVectorDatabase
from .llm import BaseLlm
from typing import List, Dict


class ChatBot:
    """
        Implements a Retrieval-Augmented Generation (RAG) system for question-answering.

        The chatbot uses a conversation history to generate a query, retrieves relevant
        context from a vector database, and uses a Large Language Model (LLM) to generate
        a precise, context-bound response based on a specialized prompt template.
    """
    def __init__(self, embedder: BaseEmbedder, vector_db: BaseVectorDatabase, llm: BaseLlm, collection_name: str):
        """
            Initializes the ChatBot with required components and the retrieval context.
            Args:
                embedder (BaseEmbedder): The service used to convert text queries into vector embeddings.
                vector_db (BaseVectorDatabase): The service used to search for and retrieve relevant documents/chunks.
                llm (BaseLlm): The service used to generate the final text response based on the prompt and context.
                collection_name (str): The name of the vector database collection containing the document chunks.
        """
        self._embedder = embedder
        self._vector_db = vector_db
        self._llm = llm
        self._collection_name = collection_name
        self._prompt_template = """
            Você é um assistente especializado em **Normas Acadêmicas do Programa de Pós-Graduação da PUC-Rio**.

            **1. Papel e Objetivo:**
            Seu único objetivo é responder à pergunta do usuário utilizando estritamente o contexto fornecido.
            
            **2. Instruções de Resposta:**
            * **Priorize a Informação do contexto:** A resposta DEVE ser baseada exclusivamente nas informações contidas no contexto. Não utilize conhecimento prévio, nem mesmo que pareça correto, se ele não estiver explicitamente suportado no texto fornecido.
            * **Clareza e Precisão:** Seja direto, claro e preciso.
            * **Mantenha o Foco:** Responda apenas o que foi perguntado.
            * **Citação:** Se a informação puder ser atribuída a um parágrafo ou seção específica no contexto, faça a citação no final da resposta, conforme o formato do contexto (exemplo: "Ver Seção 3.2 do documento Regimento.pdf").
            * **Caso de Informação Ausente:** Se a resposta para a pergunta não puder ser encontrada **de forma alguma** no contexto fornecidos, você deve responder educadamente: "Não encontrei informações suficientes sobre este tópico nos documentos normativos fornecidos. Por favor, reformule a pergunta ou consulte o site do Departamento de Informática ou a Secretaria do Programa de Pós-Graduação da PUC-Rio."
            
            **3. Fonte de Informação (Contexto):**
            O contexto abaixo é derivado de documentos oficiais das normas do programa:
            ---
            {chunks}
            ---
            
            **4. Pergunta do Usuário:**
            {query}
            
            **5. Resposta:**
        """
        self._chunk_template = """
            Contexto: {context}
            Fonte: {source}
        """

    async def interact(self, messages: List[Dict[str, str]]) -> str:
        """
            Processes a conversation history to generate a context-aware response using RAG.
            The interaction sequence is:
            1. Query Generation: Concatenates the last 5 messages to form a single text message for the query.
            2. Embedding: Generates a vector embedding for the combined query text.
            3. Retrieval: Searches the vector database for relevant text chunks using the query vector.
            4. Prompt Formatting: Inserts the retrieved chunks and the user's query into the specialized prompt template.
            5. Completion: Sends the complete prompt to the LLM for final answer generation.
            Args:
                messages (List[Dict[str, str]]): The conversation history, where each element is a dict
                                                 with 'role' and 'content' keys.
            Returns:
                str: The final, context-based answer generated by the Large Language Model.
        """
        # embed last 3 text messages
        text_message = "\n".join([f'{msg["role"]}: {msg["content"]}' for msg in messages[-5:]])
        vector = await self._embedder.embed([text_message], is_query=True)

        # retrieve relevant chunks
        chunks = await self._vector_db.retrieve(collection_name=self._collection_name, query_vector=vector[0])
        chunks_with_source = [self._chunk_template.format(context=c.payload["chunk_text"], source=c.payload["document_name"]) for c in chunks]

        # format prompt
        prompt = self._prompt_template.format(
            chunks="\n".join(chunks_with_source),
            query=text_message
        )
        # llm complete
        response = await self._llm.complete(msg=prompt, model="gpt-5-mini")
        return response
